{"paragraphs":[{"text":"%md\n----\nWhen you open this sample notebook for the first time, you will see a **Settings** section at the start. You must click **Save** in the section so that the interpreters can be bound to this notebook.","dateUpdated":"2016-09-27T00:08:53+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/scala","editorHide":true,"tableHide":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<p>When you open this sample notebook for the first time, you will see a <strong>Settings</strong> section at the start. You must click <strong>Save</strong> in the section so that the interpreters can be bound to this notebook.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<p>When you open this sample notebook for the first time, you will see a <strong>Settings</strong> section at the start. You must click <strong>Save</strong> in the section so that the interpreters can be bound to this notebook.</p>\n"},"apps":[],"jobName":"paragraph_1474673556967_-1330624892","id":"20160923-233236_360697829","dateCreated":"2016-09-23T11:32:36+0000","dateStarted":"2016-09-27T00:08:52+0000","dateFinished":"2016-09-27T00:08:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2568"},{"text":"%md\n# Working with storage account\n\n An HDInsight cluster can have a default storage and additional storage. For WASB, The URL to access the cluster storage is:\n\n    wasb[s]://<container_name>@<storage_account_name>.blob.core.windows.net/<path>\n    \nThe URL to access only the default storage is:\n\n    wasb[s]:///<path>\n\nThis notebook provides examples of how to read data from WASB into a Spark context and then perform operations on that data. The notebook also provides examples of how to write the output of Spark jobs directly into a WASB location.\n","dateUpdated":"2016-09-22T11:28:23+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Working with storage account\n\n An HDInsight cluster can have a default storage and additional storage. The URL to access the cluster storage is:</p>\n<pre><code>wasb[s]://&lt;container_name&gt;@&lt;storage_account_name&gt;.blob.core.windows.net/&lt;path&gt;\n</code></pre>\n<p>The URL to access only the default storage is:</p>\n<pre><code>wasb[s]:///&lt;path&gt;\n</code></pre>\n<p>This notebook provides examples of how to read data from WASB into a Spark context and then perform operations on that data. The notebook also provides examples of how to write the output of Spark jobs directly into a WASB location.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Working with storage account\n\n An HDInsight cluster can have a default storage and additional storage. The URL to access the cluster storage is:</p>\n<pre><code>wasb[s]://&lt;container_name&gt;@&lt;storage_account_name&gt;.blob.core.windows.net/&lt;path&gt;\n</code></pre>\n<p>The URL to access only the default storage is:</p>\n<pre><code>wasb[s]:///&lt;path&gt;\n</code></pre>\n<p>This notebook provides examples of how to read data from WASB into a Spark context and then perform operations on that data. The notebook also provides examples of how to write the output of Spark jobs directly into a WASB location.</p>\n"},"apps":[],"jobName":"paragraph_1474585898753_-657855081","id":"20160922-231138_357192970","dateCreated":"2016-09-22T11:11:38+0000","dateStarted":"2016-09-22T11:28:23+0000","dateFinished":"2016-09-22T11:28:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2569"},{"text":"%md\n-------\n##For WASB as default storage account, To Read data from WASB into Spark\n\nThe examples below read from the default storage account associated with your Spark cluster so the URL used in the examples is `/<path>`. However, you can also read from an additional storage account with the following syntax:\n\n    wasb[s]://<containername>@<accountname>.blob.core.windows.net/<path>","dateUpdated":"2016-09-22T11:28:25+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<h2>For WASB as default storage account, To Read data from WASB into Spark</h2>\n<p>The examples below read from the default storage account associated with your Spark cluster so the URL used in the examples is <code>/&lt;path&gt;</code>. However, you can also read from an additional storage account with the following syntax:</p>\n<pre><code>wasb[s]://&lt;containername&gt;@&lt;accountname&gt;.blob.core.windows.net/&lt;path&gt;\n</code></pre>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<h2>For WASB as default storage account, To Read data from WASB into Spark</h2>\n<p>The examples below read from the default storage account associated with your Spark cluster so the URL used in the examples is <code>/&lt;path&gt;</code>. However, you can also read from an additional storage account with the following syntax:</p>\n<pre><code>wasb[s]://&lt;containername&gt;@&lt;accountname&gt;.blob.core.windows.net/&lt;path&gt;\n</code></pre>\n"},"apps":[],"jobName":"paragraph_1474585918476_-996418789","id":"20160922-231158_1766984884","dateCreated":"2016-09-22T11:11:58+0000","dateStarted":"2016-09-22T11:28:25+0000","dateFinished":"2016-09-22T11:28:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2570"},{"text":"%md\n----------\n## Notebook setup\n\nWhen using Livy PySpark interpreter on HDInsight, there is no need to create a SparkContext or a SparkSession; those are all created for you automatically when you run the first code cell. The contexts are created with the following variable names:\n- SparkSession (spark)\n\nTo run the cells below, place the cursor in the cell and then press **SHIFT + ENTER**.","dateUpdated":"2016-09-22T11:28:28+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<h2>Notebook setup</h2>\n<p>When using Livy PySpark interpreter on HDInsight, there is no need to create a SparkContext or a SparkSession; those are all created for you automatically when you run the first code cell. The contexts are created with the following variable names:</p>\n<ul>\n<li>SparkSession (spark)</li>\n</ul>\n<p>To run the cells below, place the cursor in the cell and then press <strong>SHIFT + ENTER</strong>.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<h2>Notebook setup</h2>\n<p>When using Livy PySpark interpreter on HDInsight, there is no need to create a SparkContext or a SparkSession; those are all created for you automatically when you run the first code cell. The contexts are created with the following variable names:</p>\n<ul>\n<li>SparkSession (spark)</li>\n</ul>\n<p>To run the cells below, place the cursor in the cell and then press <strong>SHIFT + ENTER</strong>.</p>\n"},"apps":[],"jobName":"paragraph_1474585937717_-32961570","id":"20160922-231217_259665595","dateCreated":"2016-09-22T11:12:17+0000","dateStarted":"2016-09-22T11:28:28+0000","dateFinished":"2016-09-22T11:28:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2571"},{"text":"%md\n### Create an RDD of strings","dateUpdated":"2016-09-22T11:28:31+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Create an RDD of strings</h3>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Create an RDD of strings</h3>\n"},"apps":[],"jobName":"paragraph_1474586010629_804126154","id":"20160922-231330_447061979","dateCreated":"2016-09-22T11:13:30+0000","dateStarted":"2016-09-22T11:28:31+0000","dateFinished":"2016-09-22T11:28:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2572"},{"text":"%livy.pyspark\n# textLines is an RDD of strings\ntextLines = spark.sparkContext.textFile('/example/data/gutenberg/ulysses.txt')","dateUpdated":"2016-09-22T11:24:49+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474586041532_1232967936","id":"20160922-231401_1669639675","dateCreated":"2016-09-22T11:14:01+0000","dateStarted":"2016-09-22T11:16:05+0000","dateFinished":"2016-09-22T11:16:39+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2573"},{"text":"%md\n### Create an RDD of key-value pairs","dateUpdated":"2016-09-22T11:28:34+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Create an RDD of key-value pairs</h3>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Create an RDD of key-value pairs</h3>\n"},"apps":[],"jobName":"paragraph_1474586064739_-594173942","id":"20160922-231424_1828110650","dateCreated":"2016-09-22T11:14:24+0000","dateStarted":"2016-09-22T11:28:34+0000","dateFinished":"2016-09-22T11:28:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2574"},{"text":"%livy.pyspark\n# seqFile is an RDD of key-value pairs\nseqFile = spark.sparkContext.sequenceFile('/example/data/people.seq')","dateUpdated":"2016-09-22T11:25:02+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474586085653_1569131205","id":"20160922-231445_1591420789","dateCreated":"2016-09-22T11:14:45+0000","dateStarted":"2016-09-22T11:16:39+0000","dateFinished":"2016-09-22T11:16:44+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2575"},{"text":"%md\n### Create a dataframe from parquet files\n\nCreate a dataframe from an input parquet file. For more information about parquet files, see [here](http://spark.apache.org/docs/2.1.0/sql-programming-guide.html#parquet-files).","dateUpdated":"2016-09-23T10:41:55+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Create a dataframe from parquet files</h3>\n<p>Create a dataframe from an input parquet file. For more information about parquet files, see <a href=\"http://spark.apache.org/docs/2.1.0/sql-programming-guide.html#parquet-files\">here</a>.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Create a dataframe from parquet files</h3>\n<p>Create a dataframe from an input parquet file. For more information about parquet files, see <a href=\"http://spark.apache.org/docs/2.1.0/sql-programming-guide.html#parquet-files\">here</a>.</p>\n"},"apps":[],"jobName":"paragraph_1474586199060_-1997136998","id":"20160922-231639_630883272","dateCreated":"2016-09-22T11:16:39+0000","dateStarted":"2016-09-23T10:41:52+0000","dateFinished":"2016-09-23T10:41:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2576"},{"text":"%livy.pyspark\n# parquetFile is a dataframe that matches the schema of the input parquet file\nparquetFile = spark.read.parquet('/example/data/people.parquet')","dateUpdated":"2016-09-22T11:25:18+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474586224228_-350401839","id":"20160922-231704_1185432219","dateCreated":"2016-09-22T11:17:04+0000","dateStarted":"2016-09-22T11:17:22+0000","dateFinished":"2016-09-22T11:17:41+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2577"},{"text":"%md\n### Create a dataframe from JSON document\n\nCreate a dataframe that matches the schema of the input JSON document.","dateUpdated":"2016-09-22T11:28:41+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Create a dataframe from JSON document</h3>\n<p>Create a dataframe that matches the schema of the input JSON document.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Create a dataframe from JSON document</h3>\n<p>Create a dataframe that matches the schema of the input JSON document.</p>\n"},"apps":[],"jobName":"paragraph_1474586242732_-1500400977","id":"20160922-231722_556316333","dateCreated":"2016-09-22T11:17:22+0000","dateStarted":"2016-09-22T11:28:41+0000","dateFinished":"2016-09-22T11:28:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2578"},{"text":"%livy.pyspark\n# jsonFile is a dataframe that matches the schema of the input JSON file\njsonFile = spark.read.json('/example/data/people.json')","dateUpdated":"2016-09-22T11:25:33+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474586258708_1053592870","id":"20160922-231738_1678779467","dateCreated":"2016-09-22T11:17:38+0000","dateStarted":"2016-09-22T11:17:56+0000","dateFinished":"2016-09-22T11:17:59+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2579"},{"text":"%md\n### Create an RDD from CSV files\n\nCSV is not a first-class input data format supported by Spark, but you can use the approach below to load data in CSV format into Spark. In the snippet below, you first load a CSV file as a text file and then apply a map operation to parse the individual lines using the standard library's CSV module.","dateUpdated":"2016-09-22T11:28:44+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Create an RDD from CSV files</h3>\n<p>CSV is not a first-class input data format supported by Spark, but you can use the approach below to load data in CSV format into Spark. In the snippet below, you first load a CSV file as a text file and then apply a map operation to parse the individual lines using the standard library's CSV module.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Create an RDD from CSV files</h3>\n<p>CSV is not a first-class input data format supported by Spark, but you can use the approach below to load data in CSV format into Spark. In the snippet below, you first load a CSV file as a text file and then apply a map operation to parse the individual lines using the standard library's CSV module.</p>\n"},"apps":[],"jobName":"paragraph_1474586276124_223704817","id":"20160922-231756_1452461608","dateCreated":"2016-09-22T11:17:56+0000","dateStarted":"2016-09-22T11:28:44+0000","dateFinished":"2016-09-22T11:28:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2580"},{"text":"%livy.pyspark\nimport csv\nfrom StringIO import StringIO\n# A helper function for getting the CSV values in a string\ndef csv_values_in_line(line):\n    sio = StringIO(line)\n    value = csv.reader(sio).next()\n    sio.close()\n    return value\n\n# csvFile is an RDD of lists, each list representing a line in the CSV file\ncsvFile = spark.sparkContext.textFile('/HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv').map(csv_values_in_line)","dateUpdated":"2016-09-22T11:25:53+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474586311205_-131941084","id":"20160922-231831_983828813","dateCreated":"2016-09-22T11:18:31+0000","dateStarted":"2016-09-22T11:19:21+0000","dateFinished":"2016-09-22T11:19:22+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2581"},{"text":"%md\n------\n## For WASB as default storage account, To Write data from Spark to WASB in different formats\n\nThe examples below show you how to write output data from Spark directly into the storage accounts associated with your Spark cluster. If you are writing to the default storage account, you can provide the output path like this:\n\n    wasb[s]:///<path>\n\nIf you are writing to additional storage accounts associated with the cluster, you must provide the output path like this:\n\n    wasb[s]://<container_name>@<storage_account_name>.blob.core.windows.net/<path>","dateUpdated":"2016-09-22T11:28:48+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<h2>For WASB as default storage account, To Write data from Spark to WASB in different formats</h2>\n<p>The examples below show you how to write output data from Spark directly into the storage accounts associated with your Spark cluster. If you are writing to the default storage account, you can provide the output path like this:</p>\n<pre><code>wasb[s]:///&lt;path&gt;\n</code></pre>\n<p>If you are writing to additional storage accounts associated with the cluster, you must provide the output path like this:</p>\n<pre><code>wasb[s]://&lt;container_name&gt;@&lt;storage_account_name&gt;.blob.core.windows.net/&lt;path&gt;\n</code></pre>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<h2>For WASB as default storage account, To Write data from Spark to WASB in different formats</h2>\n<p>The examples below show you how to write output data from Spark directly into the storage accounts associated with your Spark cluster. If you are writing to the default storage account, you can provide the output path like this:</p>\n<pre><code>wasb[s]:///&lt;path&gt;\n</code></pre>\n<p>If you are writing to additional storage accounts associated with the cluster, you must provide the output path like this:</p>\n<pre><code>wasb[s]://&lt;container_name&gt;@&lt;storage_account_name&gt;.blob.core.windows.net/&lt;path&gt;\n</code></pre>\n"},"apps":[],"jobName":"paragraph_1474586361461_909466919","id":"20160922-231921_1534673440","dateCreated":"2016-09-22T11:19:21+0000","dateStarted":"2016-09-22T11:28:48+0000","dateFinished":"2016-09-22T11:28:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2582"},{"text":"%md\n### Save an RDD as text files\n\nIf you have an RDD, you can convert it to a text file like the following:","dateUpdated":"2016-09-22T11:28:50+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Save an RDD as text files</h3>\n<p>If you have an RDD, you can convert it to a text file like the following:</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Save an RDD as text files</h3>\n<p>If you have an RDD, you can convert it to a text file like the following:</p>\n"},"apps":[],"jobName":"paragraph_1474586390949_1156136937","id":"20160922-231950_1519596061","dateCreated":"2016-09-22T11:19:50+0000","dateStarted":"2016-09-22T11:28:50+0000","dateFinished":"2016-09-22T11:28:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2583"},{"text":"%livy.pyspark\n# textLines is an RDD converted into a text file\ntextLines.saveAsTextFile('/example/data/gutenberg/ulysses2py.txt')","dateUpdated":"2016-09-22T11:26:22+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474586406700_2011464390","id":"20160922-232006_1784273850","dateCreated":"2016-09-22T11:20:06+0000","dateStarted":"2016-09-22T11:20:23+0000","dateFinished":"2016-09-22T11:20:28+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2584"},{"text":"%md\n### Save a dataframe as text files\n\nIf you have a dataframe that you want to save as a text file, you must first convert it to an RDD and then save that RDD as a text file.","dateUpdated":"2016-09-22T11:28:55+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Save a dataframe as text files</h3>\n<p>If you have a dataframe that you want to save as a text file, you must first convert it to an RDD and then save that RDD as a text file.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Save a dataframe as text files</h3>\n<p>If you have a dataframe that you want to save as a text file, you must first convert it to an RDD and then save that RDD as a text file.</p>\n"},"apps":[],"jobName":"paragraph_1474586423348_182584302","id":"20160922-232023_913240581","dateCreated":"2016-09-22T11:20:23+0000","dateStarted":"2016-09-22T11:28:55+0000","dateFinished":"2016-09-22T11:28:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2585"},{"text":"%livy.pyspark\nparquetRDD = parquetFile.rdd\nparquetRDD.saveAsTextFile('/example/data/peoplepy.txt')\n# parquetFile is a dataframe converted into RDD. parquetRDD is then converted into a text file","dateUpdated":"2016-09-22T11:26:42+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474586439934_-1803858945","id":"20160922-232039_1534153975","dateCreated":"2016-09-22T11:20:39+0000","dateStarted":"2016-09-22T11:21:01+0000","dateFinished":"2016-09-22T11:21:04+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2586"},{"text":"%md\n### Save a dataframe as parquet or JSON\n\nIf you have a dataframe, you can save it to Parquet or JSON with the `.write.parquet()` and `.write.json()` methods respectively.","dateUpdated":"2016-09-22T11:28:58+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Save a dataframe as parquet or JSON</h3>\n<p>If you have a dataframe, you can save it to Parquet or JSON with the <code>.write.parquet()</code> and <code>.write.json()</code> methods respectively.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Save a dataframe as parquet or JSON</h3>\n<p>If you have a dataframe, you can save it to Parquet or JSON with the <code>.write.parquet()</code> and <code>.write.json()</code> methods respectively.</p>\n"},"apps":[],"jobName":"paragraph_1474586461196_-363697840","id":"20160922-232101_1900424995","dateCreated":"2016-09-22T11:21:01+0000","dateStarted":"2016-09-22T11:28:58+0000","dateFinished":"2016-09-22T11:28:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2587"},{"text":"%livy.pyspark\nparquetFile.write.parquet('/example/data/people2py.parquet')\njsonFile.write.json('/example/data/people2py.json')","dateUpdated":"2016-09-22T11:26:55+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474586483084_-432400971","id":"20160922-232123_766639319","dateCreated":"2016-09-22T11:21:23+0000","dateStarted":"2016-09-22T11:21:45+0000","dateFinished":"2016-09-22T11:21:50+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2588"},{"text":"%md\nNote that since both parquetFile and jsonFile are dataframes, we can save them in any format, regardless of the input format.\n","dateUpdated":"2016-09-22T11:29:01+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Note that since both parquetFile and jsonFile are dataframes, we can save them in any format, regardless of the input format.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<p>Note that since both parquetFile and jsonFile are dataframes, we can save them in any format, regardless of the input format.</p>\n"},"apps":[],"jobName":"paragraph_1474586505516_2147386376","id":"20160922-232145_2024995062","dateCreated":"2016-09-22T11:21:45+0000","dateStarted":"2016-09-22T11:29:01+0000","dateFinished":"2016-09-22T11:29:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2589"},{"text":"%livy.pyspark\nparquetFile.write.json('/example/data/people3py.json')\njsonFile.write.parquet('/example/data/people3py.parquet')","dateUpdated":"2016-09-22T11:27:12+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474586519668_-763223729","id":"20160922-232159_1777349513","dateCreated":"2016-09-22T11:21:59+0000","dateStarted":"2016-09-22T11:22:23+0000","dateFinished":"2016-09-22T11:22:26+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2590"},{"text":"%md\nIf you have an RDD and want to save it as a parquet file or JSON file, you'll have to \nconvert it to a dataframe. See [Interoperating with RDDs](http://spark.apache.org/docs/2.1.0/sql-programming-guide.html#interoperating-with-rdds) for more information.","dateUpdated":"2016-09-23T10:42:33+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>If you have an RDD and want to save it as a parquet file or JSON file, you'll have to\n<br  />convert it to a dataframe. See <a href=\"http://spark.apache.org/docs/2.1.0/sql-programming-guide.html#interoperating-with-rdds\">Interoperating with RDDs</a> for more information.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<p>If you have an RDD and want to save it as a parquet file or JSON file, you'll have to\n<br  />convert it to a dataframe. See <a href=\"http://spark.apache.org/docs/2.1.0/sql-programming-guide.html#interoperating-with-rdds\">Interoperating with RDDs</a> for more information.</p>\n"},"apps":[],"jobName":"paragraph_1474586536148_1584544708","id":"20160922-232216_1828784908","dateCreated":"2016-09-22T11:22:16+0000","dateStarted":"2016-09-23T10:42:18+0000","dateFinished":"2016-09-23T10:42:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2591"},{"text":"%md\n### Save an RDD of key-value pairs as a sequence file","dateUpdated":"2016-09-22T11:29:07+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Save an RDD of key-value pairs as a sequence file</h3>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Save an RDD of key-value pairs as a sequence file</h3>\n"},"apps":[],"jobName":"paragraph_1474586561868_-744878375","id":"20160922-232241_1029017735","dateCreated":"2016-09-22T11:22:41+0000","dateStarted":"2016-09-22T11:29:07+0000","dateFinished":"2016-09-22T11:29:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2592"},{"text":"%livy.pyspark\n# If your RDD isn't made up of key-value pairs then you'll get a runtime error\nseqFile.saveAsSequenceFile('/example/data/people2py.seq')","dateUpdated":"2016-09-22T11:27:53+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","tableHide":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474586576820_627166851","id":"20160922-232256_1124645360","dateCreated":"2016-09-22T11:22:56+0000","dateStarted":"2016-09-22T11:27:47+0000","dateFinished":"2016-09-22T11:27:48+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2593"},{"text":"%md\n### Save an RDD as a CSV\n\nBecause CSV is not natively supported by Spark, so there is no built-in way to write an RDD to a CSV file. However, you can work around this if you want to save your data as CSV. The approach below uses a helper function.","dateUpdated":"2016-09-22T11:29:10+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Save an RDD as a CSV</h3>\n<p>Because CSV is not natively supported by Spark, so there is no built-in way to write an RDD to a CSV file. However, you can work around this if you want to save your data as CSV. The approach below uses a helper function.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Save an RDD as a CSV</h3>\n<p>Because CSV is not natively supported by Spark, so there is no built-in way to write an RDD to a CSV file. However, you can work around this if you want to save your data as CSV. The approach below uses a helper function.</p>\n"},"apps":[],"jobName":"paragraph_1474586591725_-824860174","id":"20160922-232311_636852006","dateCreated":"2016-09-22T11:23:11+0000","dateStarted":"2016-09-22T11:29:10+0000","dateFinished":"2016-09-22T11:29:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2594"},{"text":"%livy.pyspark\ndef csv_string_for_row(row):\n    sio = StringIO()\n    csv.writer(sio).writerow(row)\n    value = sio.getvalue()\n    sio.close()\n    return value\ncsvFile.map(csv_string_for_row).saveAsTextFile('/example/data/HVAC2py.csv')","dateUpdated":"2016-09-22T11:28:07+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474586605669_-1757491508","id":"20160922-232325_2017152729","dateCreated":"2016-09-22T11:23:25+0000","dateStarted":"2016-09-22T11:23:44+0000","dateFinished":"2016-09-22T11:23:46+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2595"},{"dateUpdated":"2016-09-22T11:28:19+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474586624124_-1798412809","id":"20160922-232344_442078867","dateCreated":"2016-09-22T11:23:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2596"}],"name":"/PySpark/02 - Read and write data from Azure Storage","id":"2BYCETN6T","angularObjects":{"2BW4JER6M:shared_process":[],"2BY6DUW66:shared_process":[],"2BVQBY2TK:shared_process":[],"2BWAGF2BV:shared_process":[],"2BW5W61RX:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}