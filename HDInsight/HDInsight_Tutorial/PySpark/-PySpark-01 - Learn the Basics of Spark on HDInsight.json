{"paragraphs":[{"text":"%md\n----\nWhen you open this sample notebook for the first time, you will see a **Settings** section at the start. You must click **Save** in the section so that the interpreters can be bound to this notebook.","dateUpdated":"2016-09-27T00:08:26+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<p>When you open this sample notebook for the first time, you will see a <strong>Settings</strong> section at the start. You must click <strong>Save</strong> in the section so that the interpreters can be bound to this notebook.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<p>When you open this sample notebook for the first time, you will see a <strong>Settings</strong> section at the start. You must click <strong>Save</strong> in the section so that the interpreters can be bound to this notebook.</p>\n"},"apps":[],"jobName":"paragraph_1474673519663_984763341","id":"20160923-233159_1197374956","dateCreated":"2016-09-23T11:31:59+0000","dateStarted":"2016-09-27T00:08:24+0000","dateFinished":"2016-09-27T00:08:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:338"},{"text":"%md\n# Basics of Spark on HDInsight\n\n[Apache Spark](https://spark.apache.org/) is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications. When you provision a Spark cluster in HDInsight, you provision Azure compute resources with Spark installed and configured. The data to be processed is stored in Azure storage. For WASB, \n\n![Spark on HDInsight](https://mysstorage.blob.core.windows.net/notebookimages/overview/SparkArchitecture.png \"Spark on HDInsight\")\n\nNow that you have created a Spark cluster, let us understand some basics of working with Spark on HDInsight. For detailed discussion on working with Spark, see [Spark Programming Guide](https://spark.apache.org/docs/2.1.0/programming-guide.html).","dateUpdated":"2016-09-23T10:41:25+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Basics of Spark on HDInsight</h1>\n<p><a href=\"https://spark.apache.org/\">Apache Spark</a> is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications. When you provision a Spark cluster in HDInsight, you provision Azure compute resources with Spark installed and configured. The data to be processed is stored in Azure storage. For WASB, </p>\n<p><img src=\"https://mysstorage.blob.core.windows.net/notebookimages/overview/SparkArchitecture.png\" alt=\"Spark on HDInsight\" title=\"Spark on HDInsight\" /></p>\n<p>Now that you have created a Spark cluster, let us understand some basics of working with Spark on HDInsight. For detailed discussion on working with Spark, see <a href=\"http://spark.apache.org/docs/2.1.0/sql-programming-guide.html\">Spark Programming Guide</a>.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Basics of Spark on HDInsight</h1>\n<p><a href=\"https://spark.apache.org/\">Apache Spark</a> is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications. When you provision a Spark cluster in HDInsight, you provision Azure compute resources with Spark installed and configured. The data to be processed is stored in Azure storage. For WASB, </p>\n<p><img src=\"https://mysstorage.blob.core.windows.net/notebookimages/overview/SparkArchitecture.png\" alt=\"Spark on HDInsight\" title=\"Spark on HDInsight\" /></p>\n<p>Now that you have created a Spark cluster, let us understand some basics of working with Spark on HDInsight. For detailed discussion on working with Spark, see <a href=\"http://spark.apache.org/docs/2.1.0/sql-programming-guide.html\">Spark Programming Guide</a>.</p>\n"},"apps":[],"jobName":"paragraph_1474568468992_1218187052","id":"20160922-182108_420808139","dateCreated":"2016-09-22T06:21:08+0000","dateStarted":"2016-09-23T10:36:22+0000","dateFinished":"2016-09-23T10:36:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:339"},{"text":"%md\n----------\n## Notebook setup\n\nWhen using Livy PySpark interpreter on HDInsight, there is no need to create a SparkContext or a SparkSession; those are all created for you automatically when you run the first code cell. The contexts are created with the following variable names:\n- SparkSession (spark)\n\nTo run the cells below, place the cursor in the cell and then press **SHIFT + ENTER**.","dateUpdated":"2016-09-22T08:39:34+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<h2>Notebook setup</h2>\n<p>When using Livy PySpark interpreter on HDInsight, there is no need to create a SparkContext or a SparkSession; those are all created for you automatically when you run the first code cell. The contexts are created with the following variable names:</p>\n<ul>\n<li>SparkSession (spark)</li>\n</ul>\n<p>To run the cells below, place the cursor in the cell and then press <strong>SHIFT + ENTER</strong>.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<h2>Notebook setup</h2>\n<p>When using Livy PySpark interpreter on HDInsight, there is no need to create a SparkContext or a SparkSession; those are all created for you automatically when you run the first code cell. The contexts are created with the following variable names:</p>\n<ul>\n<li>SparkSession (spark)</li>\n</ul>\n<p>To run the cells below, place the cursor in the cell and then press <strong>SHIFT + ENTER</strong>.</p>\n"},"apps":[],"jobName":"paragraph_1474568518674_-703187658","id":"20160922-182158_146753265","dateCreated":"2016-09-22T06:21:58+0000","dateStarted":"2016-09-22T08:39:30+0000","dateFinished":"2016-09-22T08:39:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:340"},{"text":"%md\r\n----------\r\n\r\n## What is an RDD?\r\n\r\nBig Data applications rely on iterative, distributed computing for faster processing of large data sets. To distribute data processing over multiple jobs, the data is typically reused or shared across jobs. To share data between  existing distributed computing systems you need to store data in some intermediate stable distributed store such as HDFS. This makes the overall computations of jobs slower.\r\n\r\n**Resilient Distributed Datasets** or RDDs address this by enabling fault-tolerant, distributed, in-memory computations.\r\n\r\n----------\r\n\r\n## How do I make an RDD?\r\n\r\nRDDs can be created from stable storage or by transforming other RDDs. Run the cells below to create RDDs from the sample data files available in the storage container associated with your Spark cluster. One such sample data file is available on the cluster at `/example/data/fruits.txt`. ","dateUpdated":"2016-09-22T08:39:44+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<h2>What is an RDD?</h2>\n<p>Big Data applications rely on iterative, distributed computing for faster processing of large data sets. To distribute data processing over multiple jobs, the data is typically reused or shared across jobs. To share data between  existing distributed computing systems you need to store data in some intermediate stable distributed store such as HDFS. This makes the overall computations of jobs slower.</p>\n<p><strong>Resilient Distributed Datasets</strong> or RDDs address this by enabling fault-tolerant, distributed, in-memory computations.</p>\n<hr />\n<h2>How do I make an RDD?</h2>\n<p>RDDs can be created from stable storage or by transforming other RDDs. Run the cells below to create RDDs from the sample data files available in the storage container associated with your Spark cluster. One such sample data file is available on the cluster at <code>/example/data/fruits.txt</code>.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<h2>What is an RDD?</h2>\n<p>Big Data applications rely on iterative, distributed computing for faster processing of large data sets. To distribute data processing over multiple jobs, the data is typically reused or shared across jobs. To share data between  existing distributed computing systems you need to store data in some intermediate stable distributed store such as HDFS. This makes the overall computations of jobs slower.</p>\n<p><strong>Resilient Distributed Datasets</strong> or RDDs address this by enabling fault-tolerant, distributed, in-memory computations.</p>\n<hr />\n<h2>How do I make an RDD?</h2>\n<p>RDDs can be created from stable storage or by transforming other RDDs. Run the cells below to create RDDs from the sample data files available in the storage container associated with your Spark cluster. One such sample data file is available on the cluster at <code>/example/data/fruits.txt</code>.</p>\n"},"apps":[],"jobName":"paragraph_1474568571105_716828587","id":"20160922-182251_1744629134","dateCreated":"2016-09-22T06:22:51+0000","dateStarted":"2016-09-22T08:39:38+0000","dateFinished":"2016-09-22T08:39:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:341"},{"text":"%livy.pyspark\nfruits = spark.sparkContext.textFile('/example/data/fruits.txt')\nyellowThings = spark.sparkContext.textFile('/example/data/yellowthings.txt')","dateUpdated":"2016-09-22T08:39:55+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474568676735_-513477447","id":"20160922-182436_848808384","dateCreated":"2016-09-22T06:24:36+0000","dateStarted":"2016-09-22T06:28:05+0000","dateFinished":"2016-09-22T06:28:41+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:342"},{"text":"%md\nFor more examples on how to create RDDs see the following notebooks available with your Spark cluster:\n\n* Read and write data from Azure Storage\n* Read and write data from Hive tables","dateUpdated":"2016-09-22T08:40:05+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>For more examples on how to create RDDs see the following notebooks available with your Spark cluster:</p>\n<ul>\n<li>Read and write data from Azure Storage</li>\n<li>Read and write data from Hive tables</li>\n</ul>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<p>For more examples on how to create RDDs see the following notebooks available with your Spark cluster:</p>\n<ul>\n<li>Read and write data from Azure Storage</li>\n<li>Read and write data from Hive tables</li>\n</ul>\n"},"apps":[],"jobName":"paragraph_1474568885066_745929941","id":"20160922-182805_1710291215","dateCreated":"2016-09-22T06:28:05+0000","dateStarted":"2016-09-22T08:40:03+0000","dateFinished":"2016-09-22T08:40:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:343"},{"text":"%md\n----------\n\n## What are RDD operations?\nRDDs support two types of operations: transformations and actions.\n\n* **Transformations** create a new dataset from an existing one. Transformations are lazy, meaning that no transformation is executed until you execute an action.\n* **Actions** return a value to the driver program after running a computation on the dataset.\n\n### RDD transformations\nFollowing are examples of some of the common transformations available. For a detailed list, see [RDD Transformations](https://spark.apache.org/docs/2.1.0/programming-guide.html#transformations)\n\nRun some transformations below to understand this better. Place the cursor in the cell and press **SHIFT + ENTER**.","dateUpdated":"2016-09-23T10:40:40+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<h2>What are RDD operations?</h2>\n<p>RDDs support two types of operations: transformations and actions.</p>\n<ul>\n<li><strong>Transformations</strong> create a new dataset from an existing one. Transformations are lazy, meaning that no transformation is executed until you execute an action.</li>\n<li><strong>Actions</strong> return a value to the driver program after running a computation on the dataset.</li>\n</ul>\n<h3>RDD transformations</h3>\n<p>Following are examples of some of the common transformations available. For a detailed list, see <a href=\"https://spark.apache.org/docs/2.1.0/programming-guide.html#transformations\">RDD Transformations</a></p>\n<p>Run some transformations below to understand this better. Place the cursor in the cell and press <strong>SHIFT + ENTER</strong>.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<h2>What are RDD operations?</h2>\n<p>RDDs support two types of operations: transformations and actions.</p>\n<ul>\n<li><strong>Transformations</strong> create a new dataset from an existing one. Transformations are lazy, meaning that no transformation is executed until you execute an action.</li>\n<li><strong>Actions</strong> return a value to the driver program after running a computation on the dataset.</li>\n</ul>\n<h3>RDD transformations</h3>\n<p>Following are examples of some of the common transformations available. For a detailed list, see <a href=\"https://spark.apache.org/docs/2.1.0/programming-guide.html#transformations\">RDD Transformations</a></p>\n<p>Run some transformations below to understand this better. Place the cursor in the cell and press <strong>SHIFT + ENTER</strong>.</p>\n"},"apps":[],"jobName":"paragraph_1474568964074_688491797","id":"20160922-182924_1902008711","dateCreated":"2016-09-22T06:29:24+0000","dateStarted":"2016-09-23T10:40:37+0000","dateFinished":"2016-09-23T10:40:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:344"},{"text":"%livy.pyspark\n# map\nfruitsReversed = fruits.map(lambda fruit: fruit[::-1])","dateUpdated":"2016-09-22T08:40:29+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474569255306_-370522711","id":"20160922-183415_1370387855","dateCreated":"2016-09-22T06:34:15+0000","dateStarted":"2016-09-22T06:35:42+0000","dateFinished":"2016-09-22T06:35:43+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:345"},{"text":"%livy.pyspark\n# filter\nshortFruits = fruits.filter(lambda fruit: len(fruit) <= 5)","dateUpdated":"2016-09-22T08:40:35+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474569342672_222329372","id":"20160922-183542_1899090922","dateCreated":"2016-09-22T06:35:42+0000","dateStarted":"2016-09-22T06:36:32+0000","dateFinished":"2016-09-22T06:36:33+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:346"},{"text":"%livy.pyspark\n# flatMap\ncharacters = fruits.flatMap(lambda fruit: list(fruit))","dateUpdated":"2016-09-22T08:40:38+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474569360330_-514449448","id":"20160922-183600_374132462","dateCreated":"2016-09-22T06:36:00+0000","dateStarted":"2016-09-22T06:37:17+0000","dateFinished":"2016-09-22T06:37:17+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:347"},{"text":"%livy.pyspark\n# union\nfruitsAndYellowThings = fruits.union(yellowThings)","dateUpdated":"2016-09-22T08:40:41+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474569368946_379337571","id":"20160922-183608_765314109","dateCreated":"2016-09-22T06:36:08+0000","dateStarted":"2016-09-22T06:37:38+0000","dateFinished":"2016-09-22T06:37:39+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:348"},{"text":"%livy.pyspark\n# intersection\nyellowFruits = fruits.intersection(yellowThings)","dateUpdated":"2016-09-22T08:40:44+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474569458747_1633921657","id":"20160922-183738_963470725","dateCreated":"2016-09-22T06:37:38+0000","dateStarted":"2016-09-22T06:37:57+0000","dateFinished":"2016-09-22T06:37:58+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:349"},{"text":"%livy.pyspark\n# distinct\ndistinctFruitsAndYellowThings = fruitsAndYellowThings.distinct()\ndistinctFruitsAndYellowThings","dateUpdated":"2016-09-22T08:40:51+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474569477690_216522033","id":"20160922-183757_522699144","dateCreated":"2016-09-22T06:37:57+0000","dateStarted":"2016-09-22T06:38:17+0000","dateFinished":"2016-09-22T06:38:18+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:350"},{"text":"%livy.pyspark\n# groupByKey\nyellowThingsByFirstLetter = yellowThings.map(lambda thing: (thing[0], thing)).groupByKey()","dateUpdated":"2016-09-22T08:40:55+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474569555715_479244235","id":"20160922-183915_1718121611","dateCreated":"2016-09-22T06:39:15+0000","dateStarted":"2016-09-22T06:40:06+0000","dateFinished":"2016-09-22T06:40:07+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:351"},{"text":"%livy.pyspark\n# reduceByKey\nnumFruitsByLength = fruits.map(lambda fruit: (len(fruit), 1)).reduceByKey(lambda x, y: x + y)","dateUpdated":"2016-09-22T08:41:01+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474569606243_-1512338665","id":"20160922-184006_1128215370","dateCreated":"2016-09-22T06:40:06+0000","dateStarted":"2016-09-22T06:42:40+0000","dateFinished":"2016-09-22T06:42:41+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:352"},{"text":"%md\n### RDD actions\nFollowing are examples of some of the common actions available. For a detailed list, see [RDD Actions](https://spark.apache.org/docs/2.1.0/programming-guide.html#actions).\n\nRun some transformations below to understand this better. Place the cursor in the cell and press **SHIFT + ENTER**.","dateUpdated":"2016-09-23T10:40:52+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>RDD actions</h3>\n<p>Following are examples of some of the common actions available. For a detailed list, see <a href=\"https://spark.apache.org/docs/2.1.0/programming-guide.html#actions\">RDD Actions</a>.</p>\n<p>Run some transformations below to understand this better. Place the cursor in the cell and press <strong>SHIFT + ENTER</strong>.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<h3>RDD actions</h3>\n<p>Following are examples of some of the common actions available. For a detailed list, see <a href=\"https://spark.apache.org/docs/2.1.0/programming-guide.html#actions\">RDD Actions</a>.</p>\n<p>Run some transformations below to understand this better. Place the cursor in the cell and press <strong>SHIFT + ENTER</strong>.</p>\n"},"apps":[],"jobName":"paragraph_1474569760562_96109307","id":"20160922-184240_1724405681","dateCreated":"2016-09-22T06:42:40+0000","dateStarted":"2016-09-23T10:40:50+0000","dateFinished":"2016-09-23T10:40:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:353"},{"text":"%livy.pyspark\n# collect\nfruitsArray = fruits.collect()\nyellowThingsArray = yellowThings.collect()\nfruitsArray","dateUpdated":"2016-09-22T08:41:19+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474569845099_-1444685800","id":"20160922-184405_1556660389","dateCreated":"2016-09-22T06:44:05+0000","dateStarted":"2016-09-22T06:48:39+0000","dateFinished":"2016-09-22T06:48:40+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:354"},{"text":"%livy.pyspark\n# count\nnumFruits = fruits.count()\nnumFruits","dateUpdated":"2016-09-22T08:41:31+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474570119196_1427863168","id":"20160922-184839_1206478731","dateCreated":"2016-09-22T06:48:39+0000","dateStarted":"2016-09-22T06:48:57+0000","dateFinished":"2016-09-22T06:48:58+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:355"},{"text":"%livy.pyspark\n# take\nfirst3Fruits = fruits.take(3)\nfirst3Fruits","dateUpdated":"2016-09-22T08:41:37+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474570137300_-1144167905","id":"20160922-184857_2121924633","dateCreated":"2016-09-22T06:48:57+0000","dateStarted":"2016-09-22T06:49:14+0000","dateFinished":"2016-09-22T06:49:15+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:356"},{"text":"%livy.pyspark\n# reduce\nletterSet = fruits.map(lambda fruit: set(fruit)).reduce(lambda x, y: x.union(y))\nletterSet","dateUpdated":"2016-09-22T08:41:43+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474570154929_-1859016038","id":"20160922-184914_893329398","dateCreated":"2016-09-22T06:49:14+0000","dateStarted":"2016-09-22T06:49:31+0000","dateFinished":"2016-09-22T06:49:32+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:357"},{"text":"%md\n------\n> **IMPORTANT**: Another important RDD action is saving the output to a file. See the **Read and write data from Azure Storage** notebook for more information.","dateUpdated":"2016-09-22T08:41:45+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<blockquote><p><strong>IMPORTANT</strong>: Another important RDD action is saving the output to a file. See the <strong>Read and write data from Azure Storage</strong> notebook for more information.</p>\n</blockquote>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<blockquote><p><strong>IMPORTANT</strong>: Another important RDD action is saving the output to a file. See the <strong>Read and write data from Azure Storage</strong> notebook for more information.</p>\n</blockquote>\n"},"apps":[],"jobName":"paragraph_1474570171115_-814191267","id":"20160922-184931_1789672664","dateCreated":"2016-09-22T06:49:31+0000","dateStarted":"2016-09-22T08:41:45+0000","dateFinished":"2016-09-22T08:41:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:358"},{"text":"%md\n----------\n\n## What is a dataframe?\n\nThe `pyspark.sql` library provides an alternative API for manipulating structured datasets, known as \"dataframes\". (Dataframes are not a Spark-specific concept but `pyspark` provides its own dedicated dataframe library.) These are different from RDDs, but you can convert an RDD into a dataframe or vice-versa, if required.\n\nSee [Spark SQL and DataFrame Guide](https://spark.apache.org/docs/2.1.0/sql-programming-guide.html#dataframes) for more information.\n\n### How do I make a dataframe?\n\nYou can load a dataframe directly from an input data source. See the following notebooks included with your Spark cluster for more information.\n\n* Read and write data from Azure Storage\n* Read and write data from Hive tables\n\nYou can also create a dataframe from an RDD by specifying the schema of the dataframe as shown in the snippet below.","dateUpdated":"2016-09-23T10:41:09+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<h2>What is a dataframe?</h2>\n<p>The <code>pyspark.sql</code> library provides an alternative API for manipulating structured datasets, known as &ldquo;dataframes&rdquo;. (Dataframes are not a Spark-specific concept but <code>pyspark</code> provides its own dedicated dataframe library.) These are different from RDDs, but you can convert an RDD into a dataframe or vice-versa, if required.</p>\n<p>See <a href=\"https://spark.apache.org/docs/2.1.0/sql-programming-guide.html#datasets-and-dataframes\">Spark SQL and DataFrame Guide</a> for more information.</p>\n<h3>How do I make a dataframe?</h3>\n<p>You can load a dataframe directly from an input data source. See the following notebooks included with your Spark cluster for more information.</p>\n<ul>\n<li>Read and write data from Azure Storage</li>\n<li>Read and write data from Hive tables</li>\n</ul>\n<p>You can also create a dataframe from an RDD by specifying the schema of the dataframe as shown in the snippet below.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<h2>What is a dataframe?</h2>\n<p>The <code>pyspark.sql</code> library provides an alternative API for manipulating structured datasets, known as &ldquo;dataframes&rdquo;. (Dataframes are not a Spark-specific concept but <code>pyspark</code> provides its own dedicated dataframe library.) These are different from RDDs, but you can convert an RDD into a dataframe or vice-versa, if required.</p>\n<p>See <a href=\"https://spark.apache.org/docs/2.1.0/sql-programming-guide.html#datasets-and-dataframes\">Spark SQL and DataFrame Guide</a> for more information.</p>\n<h3>How do I make a dataframe?</h3>\n<p>You can load a dataframe directly from an input data source. See the following notebooks included with your Spark cluster for more information.</p>\n<ul>\n<li>Read and write data from Azure Storage</li>\n<li>Read and write data from Hive tables</li>\n</ul>\n<p>You can also create a dataframe from an RDD by specifying the schema of the dataframe as shown in the snippet below.</p>\n"},"apps":[],"jobName":"paragraph_1474570192451_611349381","id":"20160922-184952_1858319157","dateCreated":"2016-09-22T06:49:52+0000","dateStarted":"2016-09-23T10:41:06+0000","dateFinished":"2016-09-23T10:41:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:359"},{"text":"%livy.pyspark\nfrom pyspark.sql.types import *\nimport csv\nfrom StringIO import StringIO\ndef csv_values_in_line(line):\n    sio = StringIO(line)\n    value = csv.reader(sio).next()\n    sio.close()\n    return value\n\nbuildings = buildings = spark.sparkContext.textFile('/HdiSamples/HdiSamples/SensorSampleData/building/building.csv').map(csv_values_in_line).filter(lambda r: r[0] != 'BuildingID').map(lambda r: (int(r[0]), r[1], int(r[2]), r[3], r[4]))\nschema = StructType([StructField('BuildingID', IntegerType(), True),\n                     StructField('BuildingMgr', StringType(), True),\n                     StructField('BuildingAge', IntegerType(), True),\n                     StructField('HVACProduct', StringType(), True),\n                     StructField('Country', StringType(), True)])\ndf = spark.createDataFrame(buildings, schema)\n","dateUpdated":"2016-09-22T08:42:24+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/text","tableHide":false,"results":[],"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"result":{"code":"SUCCESS","type":"TEXT","msg":""},"apps":[],"jobName":"paragraph_1474570272530_-457624270","id":"20160922-185112_388387365","dateCreated":"2016-09-22T06:51:12+0000","dateStarted":"2016-09-22T08:42:14+0000","dateFinished":"2016-09-22T08:42:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:360"},{"text":"%md\n### Dataframe operations\n\nRun the cells below to see examples of some of the the operations that you can perform on dataframes.","dateUpdated":"2016-09-22T08:49:39+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Dataframe operations</h3>\n<p>Run the cells below to see examples of some of the the operations that you can perform on dataframes.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Dataframe operations</h3>\n<p>Run the cells below to see examples of some of the the operations that you can perform on dataframes.</p>\n"},"apps":[],"jobName":"paragraph_1474576302117_-1695920328","id":"20160922-203142_228964575","dateCreated":"2016-09-22T08:31:42+0000","dateStarted":"2016-09-22T08:49:32+0000","dateFinished":"2016-09-22T08:49:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:361"},{"text":"%livy.pyspark\n# show the content of the dataframe\ndf.show()","dateUpdated":"2016-09-22T08:49:46+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474570294732_1283996083","id":"20160922-185134_1042852543","dateCreated":"2016-09-22T06:51:34+0000","dateStarted":"2016-09-22T08:32:43+0000","dateFinished":"2016-09-22T08:32:45+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:362"},{"text":"%livy.pyspark\n# Print the dataframe schema in a tree format\ndf.printSchema()","dateUpdated":"2016-09-22T08:49:56+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474570355275_-795663667","id":"20160922-185235_1394614357","dateCreated":"2016-09-22T06:52:35+0000","dateStarted":"2016-09-22T08:33:09+0000","dateFinished":"2016-09-22T08:33:10+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:363"},{"text":"%livy.pyspark\n# Create an RDD from the dataframe\ndfrdd = df.rdd\ndfrdd.take(3)","dateUpdated":"2016-09-22T09:24:07+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474576389529_272209859","id":"20160922-203309_1865129436","dateCreated":"2016-09-22T08:33:09+0000","dateStarted":"2016-09-22T08:33:29+0000","dateFinished":"2016-09-22T08:33:31+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:364"},{"text":"%livy.pyspark\n# Retrieve a given number of rows from the dataframe\ndf.limit(3).show()","dateUpdated":"2016-09-22T09:24:12+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474576409866_2071341688","id":"20160922-203329_1229501914","dateCreated":"2016-09-22T08:33:29+0000","dateStarted":"2016-09-22T08:34:29+0000","dateFinished":"2016-09-22T08:34:30+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:365"},{"text":"%livy.pyspark\n# Retrieve specific columns from the dataframe\ndf.select('BuildingID', 'Country').limit(3).show()","dateUpdated":"2016-09-22T09:24:18+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474576469484_1171306741","id":"20160922-203429_1176552371","dateCreated":"2016-09-22T08:34:29+0000","dateStarted":"2016-09-22T08:34:45+0000","dateFinished":"2016-09-22T08:34:46+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:366"},{"text":"%livy.pyspark\n# Use GroupBy clause with dataframe \ndf.groupBy('HVACProduct').count().select('HVACProduct', 'count').show()","dateUpdated":"2016-09-22T09:24:25+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474576485377_-944027387","id":"20160922-203445_1020175946","dateCreated":"2016-09-22T08:34:45+0000","dateStarted":"2016-09-22T08:35:04+0000","dateFinished":"2016-09-22T08:35:10+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:367"},{"text":"%md\n------\n> **IMPORTANT**: Many of the methods available on normal RDDs are also available on dataframes. For example, `distinct`, `count`, `collect`, `filter`, `map`, and `take` are all methods on dataframes as well as on RDDs.","dateUpdated":"2016-09-22T09:24:33+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<blockquote><p><strong>IMPORTANT</strong>: Many of the methods available on normal RDDs are also available on dataframes. For example, <code>distinct</code>, <code>count</code>, <code>collect</code>, <code>filter</code>, <code>map</code>, and <code>take</code> are all methods on dataframes as well as on RDDs.</p>\n</blockquote>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<blockquote><p><strong>IMPORTANT</strong>: Many of the methods available on normal RDDs are also available on dataframes. For example, <code>distinct</code>, <code>count</code>, <code>collect</code>, <code>filter</code>, <code>map</code>, and <code>take</code> are all methods on dataframes as well as on RDDs.</p>\n</blockquote>\n"},"apps":[],"jobName":"paragraph_1474576504154_-2086716296","id":"20160922-203504_1822119226","dateCreated":"2016-09-22T08:35:04+0000","dateStarted":"2016-09-22T09:24:27+0000","dateFinished":"2016-09-22T09:24:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:368"},{"dateUpdated":"2016-09-22T08:38:52+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474576611762_-940043625","id":"20160922-203651_1218144194","dateCreated":"2016-09-22T08:36:51+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:369"}],"name":"/PySpark/01 - Learn the Basics of Spark on HDInsight","id":"2BVKC6BR6","angularObjects":{"2BW4JER6M:shared_process":[],"2BY6DUW66:shared_process":[],"2BVQBY2TK:shared_process":[],"2BWAGF2BV:shared_process":[],"2BW5W61RX:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}