{"paragraphs":[{"text":"%md\n----\nWhen you open this sample notebook for the first time, you will see a **Settings** section at the start. You must click **Save** in the section so that the interpreters can be bound to this notebook.","dateUpdated":"2016-09-27T00:09:13+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<p>When you open this sample notebook for the first time, you will see a <strong>Settings</strong> section at the start. You must click <strong>Save</strong> in the section so that the interpreters can be bound to this notebook.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<p>When you open this sample notebook for the first time, you will see a <strong>Settings</strong> section at the start. You must click <strong>Save</strong> in the section so that the interpreters can be bound to this notebook.</p>\n"},"apps":[],"jobName":"paragraph_1474673581663_659543251","id":"20160923-233301_157982081","dateCreated":"2016-09-23T11:33:01+0000","dateStarted":"2016-09-27T00:09:12+0000","dateFinished":"2016-09-27T00:09:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4665"},{"text":"%md\n# Analyze logs in Spark using a custom library\n\nThis notebook demonstrates how to analyze log data using a custom library with Spark on HDInsight. The custom library we use is a Python library called **iislogparser.py**. This library is already included on the Spark cluster at **/HdiSamples/HdiSamples/WebsiteLogSampleData/iislogparser.py**.","dateUpdated":"2016-09-23T05:05:16+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Analyze logs in Spark using a custom library</h1>\n<p>This notebook demonstrates how to analyze log data using a custom library with Spark on HDInsight. The custom library we use is a Python library called <strong>iislogparser.py</strong>. This library is already included on the Spark cluster at <strong>/HdiSamples/HdiSamples/WebsiteLogSampleData/iislogparser.py</strong>.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Analyze logs in Spark using a custom library</h1>\n<p>This notebook demonstrates how to analyze log data using a custom library with Spark on HDInsight. The custom library we use is a Python library called <strong>iislogparser.py</strong>. This library is already included on the Spark cluster at <strong>/HdiSamples/HdiSamples/WebsiteLogSampleData/iislogparser.py</strong>.</p>\n"},"apps":[],"jobName":"paragraph_1474646490793_2116363748","id":"20160923-160130_1505723791","dateCreated":"2016-09-23T04:01:30+0000","dateStarted":"2016-09-23T05:05:14+0000","dateFinished":"2016-09-23T05:05:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4666"},{"text":"%md\n----------\n## Notebook setup\n\nWhen using PySpark kernel notebooks on HDInsight, there is no need to create a SparkContext or a SparkSession; those are all created for you automatically when you run the first code cell, and you'll be able to see the progress printed. The contexts are created with the following variable names:\n- SparkSession (spark)\n\nTo run the cells below, place the cursor in the cell and then press **SHIFT + ENTER**.","dateUpdated":"2016-09-23T05:05:20+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<h2>Notebook setup</h2>\n<p>When using PySpark kernel notebooks on HDInsight, there is no need to create a SparkContext or a SparkSession; those are all created for you automatically when you run the first code cell, and you'll be able to see the progress printed. The contexts are created with the following variable names:</p>\n<ul>\n<li>SparkSession (spark)</li>\n</ul>\n<p>To run the cells below, place the cursor in the cell and then press <strong>SHIFT + ENTER</strong>.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<h2>Notebook setup</h2>\n<p>When using PySpark kernel notebooks on HDInsight, there is no need to create a SparkContext or a SparkSession; those are all created for you automatically when you run the first code cell, and you'll be able to see the progress printed. The contexts are created with the following variable names:</p>\n<ul>\n<li>SparkSession (spark)</li>\n</ul>\n<p>To run the cells below, place the cursor in the cell and then press <strong>SHIFT + ENTER</strong>.</p>\n"},"apps":[],"jobName":"paragraph_1474646642975_-954703617","id":"20160923-160402_1001599235","dateCreated":"2016-09-23T04:04:02+0000","dateStarted":"2016-09-23T05:05:19+0000","dateFinished":"2016-09-23T05:05:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4667"},{"text":"%md\n--------\n\n## Save raw data as an RDD\n\nStart with importing some types that are going to be used later in this sample.","dateUpdated":"2016-09-23T05:05:25+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<h2>Save raw data as an RDD</h2>\n<p>Start with importing some types that are going to be used later in this sample.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<h2>Save raw data as an RDD</h2>\n<p>Start with importing some types that are going to be used later in this sample.</p>\n"},"apps":[],"jobName":"paragraph_1474646655319_-265972437","id":"20160923-160415_406588452","dateCreated":"2016-09-23T04:04:15+0000","dateStarted":"2016-09-23T05:05:23+0000","dateFinished":"2016-09-23T05:05:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4668"},{"text":"%livy.pyspark\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *","dateUpdated":"2016-09-23T05:04:54+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474646667486_-1893845034","id":"20160923-160427_1604395858","dateCreated":"2016-09-23T04:04:27+0000","dateStarted":"2016-09-23T04:04:46+0000","dateFinished":"2016-09-23T04:04:47+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4669"},{"text":"%md\nCreate an RDD using the sample log data already available on the cluster. You can access the data in the default storage account associated with the cluster at **\\HdiSamples\\HdiSamples\\WebsiteLogSampleData\\SampleLog\\909f2b.log**.","dateUpdated":"2016-09-23T05:06:49+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Create an RDD using the sample log data already available on the cluster. You can access the data in the default storage account associated with the cluster at <strong>\\HdiSamples\\HdiSamples\\WebsiteLogSampleData\\SampleLog\\909f2b.log</strong>.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<p>Create an RDD using the sample log data already available on the cluster. You can access the data in the default storage account associated with the cluster at <strong>\\HdiSamples\\HdiSamples\\WebsiteLogSampleData\\SampleLog\\909f2b.log</strong>.</p>\n"},"apps":[],"jobName":"paragraph_1474646686214_955635969","id":"20160923-160446_63601494","dateCreated":"2016-09-23T04:04:46+0000","dateStarted":"2016-09-23T05:05:30+0000","dateFinished":"2016-09-23T05:05:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4670"},{"text":"%livy.pyspark\nlogs = spark.sparkContext.textFile('/HdiSamples/HdiSamples/WebsiteLogSampleData/SampleLog/909f2b.log')","dateUpdated":"2016-09-23T05:05:37+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/text","results":[],"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"result":{"code":"SUCCESS","type":"TEXT","msg":""},"apps":[],"jobName":"paragraph_1474646699582_1262696240","id":"20160923-160459_2027803004","dateCreated":"2016-09-23T04:04:59+0000","dateStarted":"2016-09-23T05:05:37+0000","dateFinished":"2016-09-23T05:05:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4671"},{"text":"%md\nRetrieve a sample log set to verify that the previous step completed successfully.","dateUpdated":"2016-09-23T05:05:45+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Retrieve a sample log set to verify that the previous step completed successfully.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<p>Retrieve a sample log set to verify that the previous step completed successfully.</p>\n"},"apps":[],"jobName":"paragraph_1474646717548_-902454387","id":"20160923-160517_442270921","dateCreated":"2016-09-23T04:05:17+0000","dateStarted":"2016-09-23T05:05:43+0000","dateFinished":"2016-09-23T05:05:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4672"},{"text":"%livy.pyspark\nlogs.take(5)","dateUpdated":"2016-09-23T05:04:38+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474646760726_1451500680","id":"20160923-160600_1717337345","dateCreated":"2016-09-23T04:06:00+0000","dateStarted":"2016-09-23T04:06:23+0000","dateFinished":"2016-09-23T04:06:24+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4673"},{"text":"%md\n--------\n\n## Analyze log data using a custom Python library\n\nIn the output above, the first couple lines include the header information and each remaining line matches the schema described in that header. Parsing such logs could be complicated. So, we use a custom Python library (**iislogparser.py**) that makes parsing such logs much easier. \n\nHowever, this is not a Python library that we can install with Pip, and it is not in the `PYTHONPATH`, we cannot use it by using an import statement like `import iislogparser`. To use this library, we must distribute it to all the worker nodes. \n\nThe first step in doing that is to copy it over to the default storage account associated with the cluster. Let us assume you copy it over to `/HdiSamples/HdiSamples/WebsiteLogSampleData/iislogparser.py`. You must then run the following snippet to distribute the library to all worker nodes in the Spark cluster. ","dateUpdated":"2016-09-23T05:05:52+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<h2>Analyze log data using a custom Python library</h2>\n<p>In the output above, the first couple lines include the header information and each remaining line matches the schema described in that header. Parsing such logs could be complicated. So, we use a custom Python library (<strong>iislogparser.py</strong>) that makes parsing such logs much easier.</p>\n<p>However, this is not a Python library that we can install with Pip, and it is not in the <code>PYTHONPATH</code>, we cannot use it by using an import statement like <code>import iislogparser</code>. To use this library, we must distribute it to all the worker nodes.</p>\n<p>The first step in doing that is to copy it over to the default storage account associated with the cluster. Let us assume you copy it over to <code>/HdiSamples/HdiSamples/WebsiteLogSampleData/iislogparser.py</code>. You must then run the following snippet to distribute the library to all worker nodes in the Spark cluster.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<h2>Analyze log data using a custom Python library</h2>\n<p>In the output above, the first couple lines include the header information and each remaining line matches the schema described in that header. Parsing such logs could be complicated. So, we use a custom Python library (<strong>iislogparser.py</strong>) that makes parsing such logs much easier.</p>\n<p>However, this is not a Python library that we can install with Pip, and it is not in the <code>PYTHONPATH</code>, we cannot use it by using an import statement like <code>import iislogparser</code>. To use this library, we must distribute it to all the worker nodes.</p>\n<p>The first step in doing that is to copy it over to the default storage account associated with the cluster. Let us assume you copy it over to <code>/HdiSamples/HdiSamples/WebsiteLogSampleData/iislogparser.py</code>. You must then run the following snippet to distribute the library to all worker nodes in the Spark cluster.</p>\n"},"apps":[],"jobName":"paragraph_1474646783466_-1163070599","id":"20160923-160623_1180903763","dateCreated":"2016-09-23T04:06:23+0000","dateStarted":"2016-09-23T05:05:50+0000","dateFinished":"2016-09-23T05:05:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4674"},{"text":"%livy.pyspark\ndefaultFS= spark.sparkContext._jsc.hadoopConfiguration().get('fs.defaultFS')\nspark.sparkContext.addPyFile(defaultFS + '/HdiSamples/HdiSamples/WebsiteLogSampleData/iislogparser.py')","dateUpdated":"2016-09-23T05:04:29+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474646807103_-1950620949","id":"20160923-160647_848303684","dateCreated":"2016-09-23T04:06:47+0000","dateStarted":"2016-09-23T04:07:09+0000","dateFinished":"2016-09-23T04:07:10+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4675"},{"text":"%md\n----\n`iislogparser` provides a function `parse_log_line` that returns `None` if a log line is a header row, and returns an instance of the `LogLine` class if it encounters a log line. Use the `LogLine` class to extract only the log lines from the RDD:","dateUpdated":"2016-09-23T05:07:29+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<p><code>iislogparser</code> provides a function <code>parse_log_line</code> that returns <code>None</code> if a log line is a header row, and returns an instance of the <code>LogLine</code> class if it encounters a log line. Use the <code>LogLine</code> class to extract only the log lines from the RDD:</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<p><code>iislogparser</code> provides a function <code>parse_log_line</code> that returns <code>None</code> if a log line is a header row, and returns an instance of the <code>LogLine</code> class if it encounters a log line. Use the <code>LogLine</code> class to extract only the log lines from the RDD:</p>\n"},"apps":[],"jobName":"paragraph_1474646829454_1042001968","id":"20160923-160709_1943421069","dateCreated":"2016-09-23T04:07:09+0000","dateStarted":"2016-09-23T05:07:26+0000","dateFinished":"2016-09-23T05:07:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4676"},{"text":"%livy.pyspark\ndef parse_line(l):\n    import iislogparser\n    return iislogparser.parse_log_line(l)\nlogLines = logs.map(parse_line).filter(lambda p: p is not None).cache()","dateUpdated":"2016-09-23T05:04:19+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474646852007_-1731036597","id":"20160923-160732_1552450100","dateCreated":"2016-09-23T04:07:32+0000","dateStarted":"2016-09-23T04:07:50+0000","dateFinished":"2016-09-23T04:07:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4677"},{"text":"%md\nRetrieve a couple of extracted log lines to verify that the step completed successfully.","dateUpdated":"2016-09-23T05:06:09+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Retrieve a couple of extracted log lines to verify that the step completed successfully.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<p>Retrieve a couple of extracted log lines to verify that the step completed successfully.</p>\n"},"apps":[],"jobName":"paragraph_1474646870798_1106517190","id":"20160923-160750_1374282152","dateCreated":"2016-09-23T04:07:50+0000","dateStarted":"2016-09-23T05:06:06+0000","dateFinished":"2016-09-23T05:06:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4678"},{"text":"%livy.pyspark\nlogLines.take(2)","dateUpdated":"2016-09-23T05:04:10+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474646887583_-1142709556","id":"20160923-160807_1214076459","dateCreated":"2016-09-23T04:08:07+0000","dateStarted":"2016-09-23T04:08:23+0000","dateFinished":"2016-09-23T04:08:24+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4679"},{"text":"%md\n----\nThe `LogLine` class, in turn, has some useful methods, like `is_error()`, which returns whether a log entry has an error code. Use this to compute the number of errors in the extracted log lines, and then log all the errors to a different file. ","dateUpdated":"2016-09-23T05:07:53+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<p>The <code>LogLine</code> class, in turn, has some useful methods, like <code>is_error()</code>, which returns whether a log entry has an error code. Use this to compute the number of errors in the extracted log lines, and then log all the errors to a different file.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<p>The <code>LogLine</code> class, in turn, has some useful methods, like <code>is_error()</code>, which returns whether a log entry has an error code. Use this to compute the number of errors in the extracted log lines, and then log all the errors to a different file.</p>\n"},"apps":[],"jobName":"paragraph_1474646903182_520406698","id":"20160923-160823_449032825","dateCreated":"2016-09-23T04:08:23+0000","dateStarted":"2016-09-23T05:07:51+0000","dateFinished":"2016-09-23T05:07:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4680"},{"text":"%livy.pyspark\nerrors = logLines.filter(lambda p: p.is_error())\nnumLines = logLines.count()\nnumErrors = errors.count()\nprint 'There are', numErrors, 'errors and', numLines, 'log entries'\nerrors.map(lambda p: str(p)).saveAsTextFile('/HdiSamples/HdiSamples/WebsiteLogSampleData/SampleLog/909f2b-2.log')","dateUpdated":"2016-09-23T05:04:00+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474646931311_-234824745","id":"20160923-160851_185373262","dateCreated":"2016-09-23T04:08:51+0000","dateStarted":"2016-09-23T04:09:09+0000","dateFinished":"2016-09-23T04:09:11+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4681"},{"text":"%md\n----\nIf you want to isolate the cause of requests that run for a long time, you might want to find the files that take the most time to serve on average. The snippet below retrieves the top 25 resources that took most time to serve a request.","dateUpdated":"2016-09-23T05:08:13+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<p>If you want to isolate the cause of requests that run for a long time, you might want to find the files that take the most time to serve on average. The snippet below retrieves the top 25 resources that took most time to serve a request.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<p>If you want to isolate the cause of requests that run for a long time, you might want to find the files that take the most time to serve on average. The snippet below retrieves the top 25 resources that took most time to serve a request.</p>\n"},"apps":[],"jobName":"paragraph_1474646949216_-1254394006","id":"20160923-160909_677798925","dateCreated":"2016-09-23T04:09:09+0000","dateStarted":"2016-09-23T05:08:11+0000","dateFinished":"2016-09-23T05:08:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4682"},{"text":"%livy.pyspark\ndef avgTimeTakenByKey(rdd):\n    return rdd.combineByKey(lambda line: (line.time_taken, 1),\n                            lambda x, line: (x[0] + line.time_taken, x[1] + 1),\n                            lambda x, y: (x[0] + y[0], x[1] + y[1])).map(lambda x: (x[0], float(x[1][0]) / float(x[1][1])))\n    \navgTimeTakenByKey(logLines.map(lambda p: (p.cs_uri_stem, p))).top(25, lambda x: x[1])","dateUpdated":"2016-09-23T05:03:51+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474646983486_1211354425","id":"20160923-160943_1992148893","dateCreated":"2016-09-23T04:09:43+0000","dateStarted":"2016-09-23T04:10:31+0000","dateFinished":"2016-09-23T04:10:32+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4683"},{"text":"%md\n----\nYou can also present this information in the form of plot. As a first step to create a plot, let us first create a hive table `AverageTime`. The table groups the logs by time to see if there were any unusual latency spikes at any particular time.","dateUpdated":"2016-09-23T05:08:27+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<p>You can also present this information in the form of plot. As a first step to create a plot, let us first create a hive table <code>AverageTime</code>. The table groups the logs by time to see if there were any unusual latency spikes at any particular time.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<p>You can also present this information in the form of plot. As a first step to create a plot, let us first create a hive table <code>AverageTime</code>. The table groups the logs by time to see if there were any unusual latency spikes at any particular time.</p>\n"},"apps":[],"jobName":"paragraph_1474647017323_1065195816","id":"20160923-161017_869170476","dateCreated":"2016-09-23T04:10:17+0000","dateStarted":"2016-09-23T05:08:27+0000","dateFinished":"2016-09-23T05:08:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4684"},{"text":"%livy.pyspark\nfrom pyspark.sql import *\navgTimeTakenByMinute = avgTimeTakenByKey(logLines.map(lambda p: (p.datetime.minute, p))).sortByKey()\nschema = StructType([StructField('Minutes', IntegerType(), True),\n                     StructField('Time', FloatType(), True)])\n                     \navgTimeTakenByMinuteDF = spark.createDataFrame(avgTimeTakenByMinute, schema)\ndfw = DataFrameWriter(avgTimeTakenByMinuteDF)\ndfw.saveAsTable('AverageTime')","dateUpdated":"2016-09-23T05:29:29+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/text","results":[],"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"result":{"code":"SUCCESS","type":"TEXT","msg":""},"apps":[],"jobName":"paragraph_1474647314170_-1529957941","id":"20160923-161514_416871809","dateCreated":"2016-09-23T04:15:14+0000","dateStarted":"2016-09-23T05:29:13+0000","dateFinished":"2016-09-23T05:29:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4685"},{"text":"%md\n----\nYou can then run the following SQL query to get all the records in the `AverageTime` table. You can run SQL queries using the `%livy.sql` interpreter. Do be aware that %livy.sql will currently start another Livy Scala session so make sure there is enough resources for that.","dateUpdated":"2016-09-23T05:08:41+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<p>You can then run the following SQL query to get all the records in the <code>AverageTime</code> table. You can run SQL queries using the <code>%livy.sql</code> interpreter. Do be aware that %livy.sql will currently start another Livy Scala session so make sure there is enough resources for that.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<p>You can then run the following SQL query to get all the records in the <code>AverageTime</code> table. You can run SQL queries using the <code>%livy.sql</code> interpreter. Do be aware that %livy.sql will currently start another Livy Scala session so make sure there is enough resources for that.</p>\n"},"apps":[],"jobName":"paragraph_1474647343367_-1368317430","id":"20160923-161543_739156027","dateCreated":"2016-09-23T04:15:43+0000","dateStarted":"2016-09-23T05:08:37+0000","dateFinished":"2016-09-23T05:08:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4686"},{"text":"%livy.sql\r\nSELECT * FROM AverageTime","dateUpdated":"2016-09-23T05:03:24+0000","config":{"colWidth":8,"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"keys":[{"name":"Minutes","index":0,"aggr":"sum"}],"values":[{"name":"Time","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"Minutes","index":0,"aggr":"sum"},"yAxis":{"name":"Time","index":1,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474648606824_-196376691","id":"20160923-163646_1379396787","dateCreated":"2016-09-23T04:36:46+0000","dateStarted":"2016-09-23T04:57:02+0000","dateFinished":"2016-09-23T04:57:12+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4687"},{"dateUpdated":"2016-09-23T05:03:08+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474649576690_-797346250","id":"20160923-165256_825807442","dateCreated":"2016-09-23T04:52:56+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4688"}],"name":"/PySpark/03 - Analyze logs with Spark using a custom library","id":"2BWWQ9TTK","angularObjects":{"2BW4JER6M:shared_process":[],"2BY6DUW66:shared_process":[],"2BVQBY2TK:shared_process":[],"2BWAGF2BV:shared_process":[],"2BW5W61RX:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}