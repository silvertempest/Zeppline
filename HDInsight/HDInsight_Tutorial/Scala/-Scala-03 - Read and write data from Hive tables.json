{"paragraphs":[{"text":"%md\n----\nWhen you open this sample notebook for the first time, you will see a **Settings** section at the start. You must click **Save** in the section so that the interpreters can be bound to this notebook.","dateUpdated":"2016-09-27T00:07:33+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<p>When you open this sample notebook for the first time, you will see a <strong>Settings</strong> section at the start. You must click <strong>Save</strong> in the section so that the interpreters can be bound to this notebook.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<p>When you open this sample notebook for the first time, you will see a <strong>Settings</strong> section at the start. You must click <strong>Save</strong> in the section so that the interpreters can be bound to this notebook.</p>\n"},"apps":[],"jobName":"paragraph_1474673439551_1308631614","id":"20160923-233039_704896120","dateCreated":"2016-09-23T11:30:39+0000","dateStarted":"2016-09-27T00:07:32+0000","dateFinished":"2016-09-27T00:07:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:10453"},{"text":"%md\r\n# Working with Hive tables using Spark\r\nYou can use the Spark execution engine to run Hive queries. In this notebook, you look at examples on how to read data from a Hive table and how to write data into Hive tables.\r\n\r\n----------\r\n## Notebook setup\r\n\r\nWhen using Spark kernel notebooks on HDInsight, there is no need to create a SparkContext or a SparkSession; those are all created for you automatically when you run the first code cell, and you'll be able to see the progress printed. The contexts are created with the following variable names:\r\n- SparkSession (spark)\r\n\r\nTo run the cells below, place the cursor in the cell and then press **SHIFT + ENTER**.\r\n\r\n-----\r\n## Reading data from Hive\r\n\r\nTo start with, let's first see what we have in our Hive store. Run the snippet below. At a minimum this should return a table called `hivesampletable`. `hivesampletable` is a default table that is included with the Spark HDInsight cluster.","dateUpdated":"2016-09-22T04:28:17+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Working with Hive tables using Spark</h1>\n<p>You can use the Spark execution engine to run Hive queries. In this notebook, you look at examples on how to read data from a Hive table and how to write data into Hive tables.</p>\n<hr />\n<h2>Notebook setup</h2>\n<p>When using Spark kernel notebooks on HDInsight, there is no need to create a SparkContext or a SparkSession; those are all created for you automatically when you run the first code cell, and you'll be able to see the progress printed. The contexts are created with the following variable names:</p>\n<ul>\n<li>SparkSession (spark)</li>\n</ul>\n<p>To run the cells below, place the cursor in the cell and then press <strong>SHIFT + ENTER</strong>.</p>\n<hr />\n<h2>Reading data from Hive</h2>\n<p>To start with, let's first see what we have in our Hive store. Run the snippet below. At a minimum this should return a table called <code>hivesampletable</code>. <code>hivesampletable</code> is a default table that is included with the Spark HDInsight cluster.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Working with Hive tables using Spark</h1>\n<p>You can use the Spark execution engine to run Hive queries. In this notebook, you look at examples on how to read data from a Hive table and how to write data into Hive tables.</p>\n<hr />\n<h2>Notebook setup</h2>\n<p>When using Spark kernel notebooks on HDInsight, there is no need to create a SparkContext or a SparkSession; those are all created for you automatically when you run the first code cell, and you'll be able to see the progress printed. The contexts are created with the following variable names:</p>\n<ul>\n<li>SparkSession (spark)</li>\n</ul>\n<p>To run the cells below, place the cursor in the cell and then press <strong>SHIFT + ENTER</strong>.</p>\n<hr />\n<h2>Reading data from Hive</h2>\n<p>To start with, let's first see what we have in our Hive store. Run the snippet below. At a minimum this should return a table called <code>hivesampletable</code>. <code>hivesampletable</code> is a default table that is included with the Spark HDInsight cluster.</p>\n"},"apps":[],"jobName":"paragraph_1474560923971_488442929","id":"20160922-161523_607666315","dateCreated":"2016-09-22T04:15:23+0000","dateStarted":"2016-09-22T04:28:13+0000","dateFinished":"2016-09-22T04:28:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:10454"},{"text":"%livy.sql\r\nSHOW TABLES","dateUpdated":"2016-09-22T04:28:23+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474499770350_1547289755","id":"20160921-231610_2028443362","dateCreated":"2016-09-21T11:16:10+0000","dateStarted":"2016-09-22T04:17:18+0000","dateFinished":"2016-09-22T04:17:20+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10455"},{"text":"%md\n----\nYou can then create dataframe from the `hivesampletable`. The snippet below creates a dataframe that you can perform any dataframe operation on. This dataframe contains all the data in the `hivesampletable`.","dateUpdated":"2016-09-23T10:33:59+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<p>You can then create dataframe from the <code>hivesampletable</code>. The snippet below creates a dataframe that you can perform any dataframe operation on. This dataframe contains all the data in the <code>hivesampletable</code>.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<p>You can then create dataframe from the <code>hivesampletable</code>. The snippet below creates a dataframe that you can perform any dataframe operation on. This dataframe contains all the data in the <code>hivesampletable</code>.</p>\n"},"apps":[],"jobName":"paragraph_1474499821732_-1207234890","id":"20160921-231701_406953725","dateCreated":"2016-09-21T11:17:01+0000","dateStarted":"2016-09-23T10:33:57+0000","dateFinished":"2016-09-23T10:33:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:10456"},{"text":"%livy.spark\n/* hivesampletabledf is a dataframe */\nval hivesampletabledf = spark.table(\"hivesampletable\")","dateUpdated":"2016-09-22T04:28:41+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474500008267_1680734682","id":"20160921-232008_1214788373","dateCreated":"2016-09-21T11:20:08+0000","dateStarted":"2016-09-22T04:18:22+0000","dateFinished":"2016-09-22T04:18:31+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10457"},{"text":"%md\n----\nInstead, if you want to run a query on the table and return only the results as a dataframe, you can do so with the `.sql()` method. In the snippet below, `hivesampletablequerydf` is a dataframe that only contains the data returned by the SQL query.","dateUpdated":"2016-09-23T10:34:12+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<p>Instead, if you want to run a query on the table and return only the results as a dataframe, you can do so with the <code>.sql()</code> method. In the snippet below, <code>hivesampletablequerydf</code> is a dataframe that only contains the data returned by the SQL query.</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<p>Instead, if you want to run a query on the table and return only the results as a dataframe, you can do so with the <code>.sql()</code> method. In the snippet below, <code>hivesampletablequerydf</code> is a dataframe that only contains the data returned by the SQL query.</p>\n"},"apps":[],"jobName":"paragraph_1474500075261_477148177","id":"20160921-232115_1019816151","dateCreated":"2016-09-21T11:21:15+0000","dateStarted":"2016-09-23T10:34:11+0000","dateFinished":"2016-09-23T10:34:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:10458"},{"text":"%livy.spark\nval hivesampletablequerydf = spark.sql(\"\"\"\nSELECT clientid, querytime, deviceplatform, querydwelltime \nFROM hivesampletable \nWHERE state = \"Washington\" AND devicemake = 'Microsoft' AND querydwelltime > 15\n\"\"\")","dateUpdated":"2016-09-22T04:29:00+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474561126321_1729160730","id":"20160922-161846_661884470","dateCreated":"2016-09-22T04:18:46+0000","dateStarted":"2016-09-22T04:19:52+0000","dateFinished":"2016-09-22T04:19:58+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10459"},{"text":"%md\r\n-------------\r\n## Writing data into Hive\r\n\r\nIf you have a dataframe that was created with a SparkSession and you want to persist that data to Hive, you can create a table and then insert the dataframe into the table: ","dateUpdated":"2016-09-22T04:29:09+0000","config":{"colWidth":8,"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr />\n<h2>Writing data into Hive</h2>\n<p>If you have a dataframe that was created with a SparkSession and you want to persist that data to Hive, you can create a table and then insert the dataframe into the table:</p>\n"}]},"result":{"code":"SUCCESS","type":"HTML","msg":"<hr />\n<h2>Writing data into Hive</h2>\n<p>If you have a dataframe that was created with a SparkSession and you want to persist that data to Hive, you can create a table and then insert the dataframe into the table:</p>\n"},"apps":[],"jobName":"paragraph_1474561147082_547627506","id":"20160922-161907_2053685841","dateCreated":"2016-09-22T04:19:07+0000","dateStarted":"2016-09-22T04:29:04+0000","dateFinished":"2016-09-22T04:29:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:10460"},{"text":"%livy.sql\nCREATE TABLE IF NOT EXISTS hivesampletablecopysc ( \n    clientid string, querytime string, market string, \n    deviceplatform string, devicemake string, devicemodel string, \n    state string, country string, querydwelltime double,\n    sessionid bigint, sessionpagevieworder bigint ) STORED AS PARQUET","dateUpdated":"2016-09-22T04:29:17+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474561228673_-1261169018","id":"20160922-162028_1670166263","dateCreated":"2016-09-22T04:20:28+0000","dateStarted":"2016-09-22T04:25:29+0000","dateFinished":"2016-09-22T04:25:41+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10461"},{"text":"%livy.spark\nhivesampletabledf.write.mode(org.apache.spark.sql.SaveMode.Append).insertInto(\"hivesampletablecopysc\")","dateUpdated":"2016-09-22T04:29:25+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474561529593_348865567","id":"20160922-162529_997263192","dateCreated":"2016-09-22T04:25:29+0000","dateStarted":"2016-09-22T04:26:08+0000","dateFinished":"2016-09-22T04:26:20+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10462"},{"text":"%livy.sql \nselect * from hivesampletablecopysc limit 10","dateUpdated":"2016-09-22T04:29:31+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"clientid","index":0,"aggr":"sum"}],"values":[{"name":"querytime","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"clientid","index":0,"aggr":"sum"},"yAxis":{"name":"querytime","index":1,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/text","results":{},"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474561568617_1024545932","id":"20160922-162608_28457117","dateCreated":"2016-09-22T04:26:08+0000","dateStarted":"2016-09-22T04:27:38+0000","dateFinished":"2016-09-22T04:27:39+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10463"},{"dateUpdated":"2016-09-22T04:29:37+0000","config":{"colWidth":8,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1474561616905_-1536899351","id":"20160922-162656_1979930031","dateCreated":"2016-09-22T04:26:56+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10464"}],"name":"/Scala/03 - Read and write data from Hive tables","id":"2BWT5Y2PF","angularObjects":{"2BW4JER6M:shared_process":[],"2BY6DUW66:shared_process":[],"2BVQBY2TK:shared_process":[],"2BWAGF2BV:shared_process":[],"2BW5W61RX:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}